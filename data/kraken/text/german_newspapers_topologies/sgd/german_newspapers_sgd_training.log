time nice ketos train -f binary  -o ./20231231/sgd/german_newspapers -d cuda:0 --lag 10 -r 0.0001 -B 4 -w 0 -s '[1,144,0,1 Cr4,2,16,1,1 Mp4,2 Cr2,2,48,1,1, Gn24 Mp2,2 Cr2,2,72,1,1 Gn36 Mp2,2 S1(1x0)1,3 Lbx288 Do0.2,2 Lbx288 Do0.2,2 Lbx288]' german_newspapers_2023_12.arrow

GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A4000 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃    ┃ Name      ┃ Type                     ┃ Params ┃                 In sizes ┃                Out sizes ┃
┡━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ 0  │ val_cer   │ CharErrorRate            │      0 │                        ? │                        ? │
│ 1  │ val_wer   │ WordErrorRate            │      0 │                        ? │                        ? │
│ 2  │ net       │ MultiParamSequential     │  6.2 M │  [[1, 1, 144, 400], '?'] │   [[1, 264, 1, 49], '?'] │
│ 3  │ net.C_0   │ ActConv2D                │    144 │  [[1, 1, 144, 400], '?'] │ [[1, 16, 143, 399], '?'] │
│ 4  │ net.Mp_1  │ MaxPool                  │      0 │ [[1, 16, 143, 399], '?'] │  [[1, 16, 35, 199], '?'] │
│ 5  │ net.C_2   │ ActConv2D                │  3.1 K │  [[1, 16, 35, 199], '?'] │  [[1, 48, 34, 198], '?'] │
│ 6  │ net.Gn_3  │ GroupNorm                │     96 │  [[1, 48, 34, 198], '?'] │  [[1, 48, 34, 198], '?'] │
│ 7  │ net.Mp_4  │ MaxPool                  │      0 │  [[1, 48, 34, 198], '?'] │   [[1, 48, 17, 99], '?'] │
│ 8  │ net.C_5   │ ActConv2D                │ 13.9 K │   [[1, 48, 17, 99], '?'] │   [[1, 72, 16, 98], '?'] │
│ 9  │ net.Gn_6  │ GroupNorm                │    144 │   [[1, 72, 16, 98], '?'] │   [[1, 72, 16, 98], '?'] │
│ 10 │ net.Mp_7  │ MaxPool                  │      0 │   [[1, 72, 16, 98], '?'] │    [[1, 72, 8, 49], '?'] │
│ 11 │ net.S_8   │ Reshape                  │      0 │    [[1, 72, 8, 49], '?'] │   [[1, 576, 1, 49], '?'] │
│ 12 │ net.L_9   │ TransposedSummarizingRNN │  2.0 M │   [[1, 576, 1, 49], '?'] │   [[1, 576, 1, 49], '?'] │
│ 13 │ net.Do_10 │ Dropout                  │      0 │   [[1, 576, 1, 49], '?'] │   [[1, 576, 1, 49], '?'] │
│ 14 │ net.L_11  │ TransposedSummarizingRNN │  2.0 M │   [[1, 576, 1, 49], '?'] │   [[1, 576, 1, 49], '?'] │
│ 15 │ net.Do_12 │ Dropout                  │      0 │   [[1, 576, 1, 49], '?'] │   [[1, 576, 1, 49], '?'] │
│ 16 │ net.L_13  │ TransposedSummarizingRNN │  2.0 M │   [[1, 576, 1, 49], '?'] │   [[1, 576, 1, 49], '?'] │
│ 17 │ net.O_14  │ LinSoftmax               │  152 K │   [[1, 576, 1, 49], '?'] │   [[1, 264, 1, 49], '?'] │
└────┴───────────┴──────────────────────────┴────────┴──────────────────────────┴──────────────────────────┘
Trainable params: 6.2 M                                                                                                                                                                
Non-trainable params: 0                                                                                                                                                                
Total params: 6.2 M                                                                                                                                                                    
Total estimated model params size (MB): 24                                                                                                                                             
stage 0/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:04:58 • 0:00:00 13.09it/s val_accuracy: 0.988 val_word_accuracy: 0.939  early_stopping: 0/10 0.98759
stage 1/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:29 • 0:00:00 12.79it/s val_accuracy: 0.991 val_word_accuracy: 0.953  early_stopping: 0/10 0.99055
stage 2/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:34 • 0:00:00 12.94it/s val_accuracy: 0.992 val_word_accuracy: 0.958  early_stopping: 0/10 0.99168
stage 3/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:33 • 0:00:00 12.86it/s val_accuracy: 0.992 val_word_accuracy: 0.962  early_stopping: 0/10 0.99236
stage 4/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:37 • 0:00:00 12.87it/s val_accuracy: 0.993 val_word_accuracy: 0.964  early_stopping: 0/10 0.99273
stage 5/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:41 • 0:00:00 12.74it/s val_accuracy: 0.993 val_word_accuracy: 0.966  early_stopping: 0/10 0.99314
stage 6/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:47 • 0:00:00 12.27it/s val_accuracy: 0.993 val_word_accuracy: 0.968  early_stopping: 0/10 0.99344
stage 7/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:48 • 0:00:00 12.95it/s val_accuracy: 0.994 val_word_accuracy: 0.968  early_stopping: 0/10 0.99355
stage 8/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:07:48 • 0:00:00 12.51it/s val_accuracy: 0.993 val_word_accuracy: 0.967  early_stopping: 1/10 0.99355
stage 9/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:48 • 0:00:00 12.70it/s val_accuracy: 0.994 val_word_accuracy: 0.969  early_stopping: 0/10 0.99366
stage 10/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:07:07 • 0:00:00 12.74it/s val_accuracy: 0.994 val_word_accuracy: 0.97  early_stopping: 0/10 0.99392
stage 11/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:43 • 0:00:00 12.55it/s val_accuracy: 0.994 val_word_accuracy: 0.969  early_stopping: 1/10 0.99392
stage 12/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:31 • 0:00:00 12.47it/s val_accuracy: 0.994 val_word_accuracy: 0.97  early_stopping: 0/10 0.99397
stage 13/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:42 • 0:00:00 12.41it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 0/10 0.99400
stage 14/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:32 • 0:00:00 12.41it/s val_accuracy: 0.994 val_word_accuracy: 0.97  early_stopping: 0/10 0.99403
stage 15/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:19 • 0:00:00 12.77it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 1/10 0.99403
stage 16/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:27 • 0:00:00 12.84it/s val_accuracy: 0.994 val_word_accuracy: 0.97  early_stopping: 2/10 0.99403
stage 17/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:25 • 0:00:00 12.74it/s val_accuracy: 0.994 val_word_accuracy: 0.97  early_stopping: 3/10 0.99403
stage 18/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:26 • 0:00:00 12.89it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 0/10 0.99414
stage 19/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:29 • 0:00:00 12.79it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 1/10 0.99414
stage 20/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:34 • 0:00:00 12.92it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 2/10 0.99414
stage 21/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:25 • 0:00:00 12.79it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 3/10 0.99414
stage 22/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:26 • 0:00:00 12.94it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 0/10 0.99416
stage 23/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:26 • 0:00:00 12.60it/s val_accuracy: 0.994 val_word_accuracy: 0.97  early_stopping: 1/10 0.99416
stage 24/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:31 • 0:00:00 12.81it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 2/10 0.99416
stage 25/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:28 • 0:00:00 12.73it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 0/10 0.99426
stage 26/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:27 • 0:00:00 12.62it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 1/10 0.99426
stage 27/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:31 • 0:00:00 12.73it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 2/10 0.99426
stage 28/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:26 • 0:00:00 12.78it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 0/10 0.99429
stage 29/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:34 • 0:00:00 11.81it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 1/10 0.99429
stage 30/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:30 • 0:00:00 12.41it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 0/10 0.99436
stage 31/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:07:38 • 0:00:00 12.34it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 1/10 0.99436
stage 32/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:08:01 • 0:00:00 12.13it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 2/10 0.99436
stage 33/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:07:04 • 0:00:00 12.69it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 3/10 0.99436
stage 34/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:15 • 0:00:00 12.95it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 4/10 0.99436
stage 35/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:07:30 • 0:00:00 11.97it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 5/10 0.99436
stage 36/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:06 • 0:00:00 12.78it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 6/10 0.99436
stage 37/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:40 • 0:00:00 12.85it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 7/10 0.99436
stage 38/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:06:42 • 0:00:00 12.65it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 8/10 0.99436
stage 39/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:41 • 0:00:00 12.94it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 9/10 0.99436
stage 40/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 1:05:43 • 0:00:00 12.49it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 10/10 0.99436
Moving best model ./20231231/sgd/german_newspapers_30.mlmodel (0.9943616986274719) to ./20231231/sgd/german_newspapers_best.mlmodel
nice ketos train -f binary -o ./20231231/sgd/german_newspapers -d cuda:0  199080,08s user 13389,96s system 123% cpu 47:37:04,78 total

