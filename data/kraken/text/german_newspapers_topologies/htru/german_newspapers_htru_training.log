time nice ketos train -f binary  -o ./20231231/htru/german_newspapers -d cuda:0 --lag 10 -r 0.0001 -B 4 -w 0 -s '[1,120,0,1 Cr4,2,32,4,2 Gn32 Cr4,2,64,1,1 Gn32 Mp4,2,4,2 Cr3,3,128,1,1 Gn32 Mp1,2,1,2 S1(1x0)1,3 Lbx256 Do0.5 Lbx256 Do0.5 Lbx256 Do0.5]' german_newspapers_2023_12.arrow
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A4000 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃    ┃ Name      ┃ Type                     ┃ Params ┃                In sizes ┃               Out sizes ┃
┡━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ 0  │ val_cer   │ CharErrorRate            │      0 │                       ? │                       ? │
│ 1  │ val_wer   │ WordErrorRate            │      0 │                       ? │                       ? │
│ 2  │ net       │ MultiParamSequential     │  5.7 M │ [[1, 1, 120, 400], '?'] │  [[1, 264, 1, 49], '?'] │
│ 3  │ net.C_0   │ ActConv2D                │    288 │ [[1, 1, 120, 400], '?'] │ [[1, 32, 30, 200], '?'] │
│ 4  │ net.Gn_1  │ GroupNorm                │     64 │ [[1, 32, 30, 200], '?'] │ [[1, 32, 30, 200], '?'] │
│ 5  │ net.C_2   │ ActConv2D                │ 16.4 K │ [[1, 32, 30, 200], '?'] │ [[1, 64, 29, 199], '?'] │
│ 6  │ net.Gn_3  │ GroupNorm                │    128 │ [[1, 64, 29, 199], '?'] │ [[1, 64, 29, 199], '?'] │
│ 7  │ net.Mp_4  │ MaxPool                  │      0 │ [[1, 64, 29, 199], '?'] │   [[1, 64, 7, 99], '?'] │
│ 8  │ net.C_5   │ ActConv2D                │ 73.9 K │   [[1, 64, 7, 99], '?'] │  [[1, 128, 7, 99], '?'] │
│ 9  │ net.Gn_6  │ GroupNorm                │    256 │  [[1, 128, 7, 99], '?'] │  [[1, 128, 7, 99], '?'] │
│ 10 │ net.Mp_7  │ MaxPool                  │      0 │  [[1, 128, 7, 99], '?'] │  [[1, 128, 7, 49], '?'] │
│ 11 │ net.S_8   │ Reshape                  │      0 │  [[1, 128, 7, 49], '?'] │  [[1, 896, 1, 49], '?'] │
│ 12 │ net.L_9   │ TransposedSummarizingRNN │  2.4 M │  [[1, 896, 1, 49], '?'] │  [[1, 512, 1, 49], '?'] │
│ 13 │ net.Do_10 │ Dropout                  │      0 │  [[1, 512, 1, 49], '?'] │  [[1, 512, 1, 49], '?'] │
│ 14 │ net.L_11  │ TransposedSummarizingRNN │  1.6 M │  [[1, 512, 1, 49], '?'] │  [[1, 512, 1, 49], '?'] │
│ 15 │ net.Do_12 │ Dropout                  │      0 │  [[1, 512, 1, 49], '?'] │  [[1, 512, 1, 49], '?'] │
│ 16 │ net.L_13  │ TransposedSummarizingRNN │  1.6 M │  [[1, 512, 1, 49], '?'] │  [[1, 512, 1, 49], '?'] │
│ 17 │ net.Do_14 │ Dropout                  │      0 │  [[1, 512, 1, 49], '?'] │  [[1, 512, 1, 49], '?'] │
│ 18 │ net.O_15  │ LinSoftmax               │  135 K │  [[1, 512, 1, 49], '?'] │  [[1, 264, 1, 49], '?'] │
└────┴───────────┴──────────────────────────┴────────┴─────────────────────────┴─────────────────────────┘
Trainable params: 5.7 M                                                                                                                                                                
Non-trainable params: 0                                                                                                                                                                
Total params: 5.7 M                                                                                                                                                                    
Total estimated model params size (MB): 22                                                                                                                                             
stage 0/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:48:05 • 0:00:00 17.44it/s val_accuracy: 0.987 val_word_accuracy: 0.935  early_stopping: 0/10 0.98660
stage 1/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:49:45 • 0:00:00 16.97it/s val_accuracy: 0.99 val_word_accuracy: 0.951  early_stopping: 0/10 0.98987
stage 2/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:49:49 • 0:00:00 16.99it/s val_accuracy: 0.991 val_word_accuracy: 0.957  early_stopping: 0/10 0.99111
stage 3/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:36 • 0:00:00 16.62it/s val_accuracy: 0.992 val_word_accuracy: 0.96  early_stopping: 0/10 0.99188
stage 4/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:32 • 0:00:00 16.26it/s val_accuracy: 0.992 val_word_accuracy: 0.964  early_stopping: 0/10 0.99247
stage 5/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:42 • 0:00:00 16.14it/s val_accuracy: 0.993 val_word_accuracy: 0.965  early_stopping: 0/10 0.99282
stage 6/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:55:55 • 0:00:00 13.90it/s val_accuracy: 0.992 val_word_accuracy: 0.963  early_stopping: 1/10 0.99282
stage 7/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:22 • 0:00:00 16.70it/s val_accuracy: 0.993 val_word_accuracy: 0.966  early_stopping: 0/10 0.99306
stage 8/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:55:13 • 0:00:00 13.74it/s val_accuracy: 0.993 val_word_accuracy: 0.967  early_stopping: 0/10 0.99325
stage 9/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:18 • 0:00:00 16.38it/s val_accuracy: 0.993 val_word_accuracy: 0.967  early_stopping: 1/10 0.99325
stage 10/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:37 • 0:00:00 16.44it/s val_accuracy: 0.993 val_word_accuracy: 0.969  early_stopping: 0/10 0.99350
stage 11/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:28 • 0:00:00 16.18it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 0/10 0.99383
stage 12/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:08 • 0:00:00 16.45it/s val_accuracy: 0.994 val_word_accuracy: 0.97  early_stopping: 1/10 0.99383
stage 13/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:30 • 0:00:00 16.49it/s val_accuracy: 0.994 val_word_accuracy: 0.969  early_stopping: 2/10 0.99383
stage 14/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:32 • 0:00:00 16.42it/s val_accuracy: 0.994 val_word_accuracy: 0.97  early_stopping: 3/10 0.99383
stage 15/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:32 • 0:00:00 16.24it/s val_accuracy: 0.994 val_word_accuracy: 0.969  early_stopping: 4/10 0.99383
stage 16/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:31 • 0:00:00 16.26it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 0/10 0.99404
stage 17/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:56:51 • 0:00:00 14.36it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 0/10 0.99408
stage 18/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:07 • 0:00:00 16.57it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 1/10 0.99408
stage 19/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:35 • 0:00:00 16.37it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 0/10 0.99420
stage 20/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:38 • 0:00:00 14.41it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 0/10 0.99424
stage 21/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:23 • 0:00:00 14.08it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 1/10 0.99424
stage 22/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:56:45 • 0:00:00 14.34it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 0/10 0.99425
stage 23/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:49 • 0:00:00 16.64it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 1/10 0.99425
stage 24/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:37 • 0:00:00 16.34it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 2/10 0.99425
stage 25/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:36 • 0:00:00 16.78it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 0/10 0.99426
stage 26/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:38 • 0:00:00 16.71it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 0/10 0.99434
stage 27/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:37 • 0:00:00 16.63it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 1/10 0.99434
stage 28/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:38 • 0:00:00 16.63it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 0/10 0.99438
stage 29/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:22 • 0:00:00 16.68it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 1/10 0.99438
stage 30/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:37 • 0:00:00 16.46it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 2/10 0.99438
stage 31/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:41 • 0:00:00 16.21it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 3/10 0.99438
stage 32/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:41 • 0:00:00 16.32it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 4/10 0.99438
stage 33/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:37 • 0:00:00 16.66it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 5/10 0.99438
stage 34/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:42 • 0:00:00 16.47it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 0/10 0.99439
stage 35/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:43 • 0:00:00 16.42it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 0/10 0.99442
stage 36/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:43 • 0:00:00 16.43it/s val_accuracy: 0.994 val_word_accuracy: 0.971  early_stopping: 1/10 0.99442
stage 37/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:54:10 • 0:00:00 16.80it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 2/10 0.99442
stage 38/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:29 • 0:00:00 16.31it/s val_accuracy: 0.994 val_word_accuracy: 0.972  early_stopping: 3/10 0.99442
stage 39/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:43 • 0:00:00 16.28it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 4/10 0.99442
stage 40/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:44 • 0:00:00 16.29it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 5/10 0.99442
stage 41/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:21 • 0:00:00 16.64it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 6/10 0.99442
stage 42/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:53:59 • 0:00:00 16.45it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 7/10 0.99442
stage 43/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:52:29 • 0:00:00 16.48it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 8/10 0.99442
stage 44/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:41 • 0:00:00 16.55it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 9/10 0.99442
stage 45/∞ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50088/50088 0:50:44 • 0:00:00 16.68it/s val_accuracy: 0.994 val_word_accuracy: 0.973  early_stopping: 10/10 0.99442
Moving best model ./20231231/htru/german_newspapers_35.mlmodel (0.9944181442260742) to ./20231231/htru/german_newspapers_best.mlmodel
nice ketos train -f binary -o ./20231231/htru/german_newspapers -d     183509,75s user 9714,60s system 128% cpu 41:43:41,61 total

